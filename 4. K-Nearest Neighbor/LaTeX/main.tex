\documentclass[a4paper,12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[margin=2.5cm, top=2cm]{geometry}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{float}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{titlesec}
\usepackage{array}
\usepackage{tabularx}
\usepackage{indentfirst}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{tcolorbox}
\usepackage{xcolor}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}

% Pengaturan warna untuk kode Python
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\renewcommand{\tabularxcolumn}[1]{m{#1}}
\newcolumntype{Y}{>{\centering\arraybackslash}X}

\lstset{
    language=Python,
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\small,
    breaklines=true,
    showstringspaces=false
}

\titleformat{\title}{\normalfont\Large\bfseries}{}{0pt}{}
\titlespacing*{\section}{0pt}{0.8\baselineskip}{0.6\baselineskip}

\title{\textbf{K-Nearest Neighbor}}
\author{Mohammad Febryan Khamim}
\date{} % kosongkan tanggal agar tidak muncul

\begin{document}

\maketitle

\section{Pengantar K-Nearest Neighbor (KNN)}

\textbf{\textit{K-Nearest Neighbor} (KNN)} adalah algoritma \textit{supervised machine learning} yang umumnya digunakan untuk klasifikasi dan regresi. 
\textbf{Cara kerjanya} adalah dengan menemukan 'k' terdekat titik (tetangga) dari suatu data input baru. 

Dari \textbf{k} tetangga terdekat tersebut, selanjutnya: 
\begin{itemize}
    \item \textbf{Prediksi} : Jumlah kelas terbanyak
    \item \textbf{Regresi} : Menentukan rata-rata \textbf{k} titik tersebut
\end{itemize}

\noindent KNN tidak mengasumsikan data terdistribusi tertentu (Normal, Gaussian, dan sebagainya) karena hanya berdasarkan jarak antardata. 
Selain itu, KNN tak perlu parameter yang banyak. 
Untuk menemukan model terbaik, hanya diperlukan sebuah langkah untuk menentukan nilai '\textbf{k}' paling optimal. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Gambar/knn.jpg}
    \caption{Ilustrasi K-Nearest Neighbors}
    \label{fig:knn}
\end{figure}

K-Nearest Neighbor disebut sebagai algoritma pembelajar yang malas (\textit{lazy learner}) karena algoritma ini tak melakukan pembelajaran langsung dari data latih. 
Algoritma ini menyimpan seluruh \textit{dataset} dan baru melakukan perhitungan atau prediksi dilakukan. 

\section{Penentuan nilai 'k'}

\begin{itemize}
    \item '\textbf{k}' adalah jumlah tetangga yang hendak diamati ketika menentukan keputusan
    \item Memilih '\textbf{k}' penting untuk memutuskan hasil terbaik
    \item Data banyak \textit{noise} atau \textit{outlier} $\rightarrow$ Semakin besar nilai \textbf{k} maka semakin baik
    \item Semakin besar nilai \textbf{k} $\rightarrow$ Model gagal menangkap pola-pola penting $\rightarrow$ \textit{underfitting}
    \item Dapat menggunakan : \textit{Cross Validation} \& \textit{Elbow Method}
    \item Lebih baik digunakan nilai \textbf{k} ganjil
\end{itemize}

\section{Kelebihan dan Kekurangan KNN}

\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|>{\raggedright\arraybackslash}p{6cm}|>{\raggedright\arraybackslash}p{6cm}|}
\hline
\multicolumn{1}{|c|}{\textbf{Kelebihan}} &
\multicolumn{1}{c|}{\textbf{Kekurangan}} \\ \hline

\begin{itemize}[leftmargin=*, itemsep=2pt, topsep=0pt, partopsep=0pt, parsep=0pt]
    \item Algoritma mudah dipahami
    \item Tak perlu proses \textit{training} yang rumit
    \item Fleksibel untuk berbagai distribusi data
    \item Mudah untuk menambahkan data
\end{itemize}
&
\begin{itemize}[leftmargin=*, itemsep=2pt, topsep=0pt, partopsep=0pt, parsep=0pt]
    \item Lambat untuk \textit{dataset} besar
    \item Butuh banyak memori
    \item Sensitif terhadap skala data
    \item Sensitif terhadap \textit{noise} / \textit{outlier}
    \item Kurang efisien untuk fitur kategorikal
\end{itemize}
\\ \hline

\end{tabular}
\end{center}

\section{Penggunaan KNN}

Algoritma dengan intuisi yang sederhana ini, dapat digunakan pada beberapa kasus, di antaranya sebagai berikut. 
\begin{itemize}
    \item Ukuran data kecil-menengah
    \item Jumlah fitur tak terlalu banyak
    \item Data bersih
\end{itemize}

\section{Metrik Jarak yang Digunakan dalam KNN}
Untuk menentukan jaraik antartitik pada KNN, terdapat beberapa metrik jarak yang digunakan, yakni sebagai berikut. 
\begin{enumerate}
    \item \textbf{Euclidean Distance}
        \begin{equation*}
            \text{d}_{\text{euc}} (x,x_i) = \sqrt{\sum^d_{j=1}(x_j - X_{ij})}
        \end{equation*}
    \item \textbf{Manhattan Distance}
        \begin{equation*}
            \text{d}_{\text{man}} (x,y) = \sum^n_{i=1} |x_i - y_i|
        \end{equation*}
    \item \textbf{Minkowski Distance}
        \begin{equation*}
            \text{d}_{\text{min}} = (\sum^n_{i=1} (x_i - y_1)^p)^{\dfrac{1}{p}}
        \end{equation*}
\end{enumerate}

\section{Algoritma KNN}
Berikut ini adalah langkah-langkah atau algoritma dalam menyelesaikan tugas ML, yakni: 
\begin{enumerate}
    \item Menentukan nilai \textbf{k} optimal
    \item Menghitung dan menentukan \textbf{k} tetangga terdekat
    \item Menghitung jarak titik saat ini dengan \textbf{k} tetangga terdekat
    \item Voting : Klasifikasi | Rata-rata : Regresi
\end{enumerate}

\section{Catatan}

\begin{itemize}
    \item Dalam KNN, data atau fitur yang memiliki \textit{range} nilai yang lebih besar akan sangat memengaruhi perhitungan jarak $\rightarrow$ butuh standardisasi.
    \item Bagaimana memilih Metrik Jarak terbaik?
        \begin{itemize}
            \item Euclidean : \textit{Default} untuk data yang bersih
            \item Manhattan : Lebih tahan \textit{outlier} dan data grid
            \item Minkowski : Fleksibel dan bisa disesuaikan
        \end{itemize}
\end{itemize}

\newpage

\section{Source Code Python}
\begin{lstlisting}[language=Python, caption=Python Code]
# Import libraries yang dibutuhkan
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline

# Load Dataset
df = pd.read_csv("Classified Data",index_col=0)
df.head()

# Standardisasi data
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(df.drop('TARGET CLASS',axis=1))
scaled_features = scaler.transform(df.drop('TARGET CLASS',axis=1))
df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])

# Train Test Split
from sklearn.model_selection import train_test_split
X = df_feat
y = df['TARGET CLASS']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)

# KNN Model
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train,y_train)
pred = knn.predict(X_test)

# Evaluasi Model
from sklearn.metrics import classification_report,confusion_matrix
print(confusion_matrix(y_test,pred))
print(classification_report(y_test,pred))

# Menentukan nilai K yang optimal
error_rate = []
for i in range(1,40):
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train,y_train)
    pred_i = knn.predict(X_test)
    error_rate.append(np.mean(pred_i != y_test))

# Plotting error rate vs K value
plt.figure(figsize=(10,6))
plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',
         markerfacecolor='red', markersize=10)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')

# Menggunakan k=1
knn = KNeighborsClassifier(n_neighbors=1)

knn.fit(X_train,y_train)
pred = knn.predict(X_test)

print('WITH K=1')
print('\n')
print(confusion_matrix(y_test,pred))
print('\n')
print(classification_report(y_test,pred))

# Menggunakan k optimal
# NOW WITH K=23
knn = KNeighborsClassifier(n_neighbors=23)

knn.fit(X_train,y_train)
pred = knn.predict(X_test)

print('WITH K=23')
print('\n')
print(confusion_matrix(y_test,pred))
print('\n')
print(classification_report(y_test,pred))
\end{lstlisting}

\end{document} 